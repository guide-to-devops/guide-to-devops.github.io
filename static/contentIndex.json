{"blog/Admission-Controller-소개":{"title":"Admission Controller 소개","links":[],"tags":["Kubernetes","Policy"],"content":"Admission Controller는 k8s 클러스터에서 관리자가 정의한 정책(Policy)을 수행(Admission Control)하는 플러그인입니다.\n\nAdmission Controller를 사용하는 이유\n\n보안성 측면: 인증 및 인가를 거친 요청에 대해 미리 정의한 정책을 적용하므로 보안성을 높일 수 있습니다.\n제어성 측면: 클러스터에 배포하는 App에 적절한 Label을 자동으로 기입하거나 App이 사용할 Memory 또는 CPU의 규모를 자동으로 제한하여, 클러스터 내 배포하는 리소스를 더욱 효과적으로 제어할 수도 있습니다.\n\nAdmission Controller 작동 방식\n위의 표처럼, k8s API 서버로 온 요청은 인증(Authentication)과 인가(Authorization) 과정을 먼저 거친 다음 해당 요청에 대한 Admission Control이 수행됩니다.\nAdmission Control은 아래와 같이 2단계로 나눠 진행됩니다.\n\n변경(Mutate) 단계: 들어온 요청과 매칭되는 정책이 해당 요청의 일부 값을 변경하는 Mutate 정책일 경우, 해당 요청에서 변경되어야 하는 부분을 알려주는 값을 k8s API 서버로 반환합니다.\n검증(Validate) 단계: 들어온 요청과 매칭되는 정책이 해당 요청을 허용/거부하는 Validate 정책일 경우, 해당 요청이 정책에 부합하는지에 대해 true/false 값을 k8s API 서버로 반환합니다.\n\nAdmission Controller를 사용하는 방법\nk8s 클러스터에는 Pod에 사용하는 Image나 인증서 등에 관한 다양한 Admission Controller가 기본적으로 준비되어 있습니다. (관련 링크) Admission Controller를 사용하려면 k8s API 서버의 config에 아래 내용을 추가합니다.\n--enable-admission-plugins={추가하려는 Plugin 이름}\n\nAdmission Controller를 직접 구현하는 방법도 있습니다. k8s API 서버로부터 오는 요청을 받아서 특정 정책에 따른 로직을 수행 후 Admission Control 결과 양식에 따라 결과값을 반환하는 서비스를 구현하는 방식인데요.\n이때 Admission Webhook이 사용되며, 이는 외부에서 작동하는 Admission Controller가 Admission Control 요청을 받고 이에 따른 특정 행위를 수행하도록 도와주는 역할을 합니다. 서드파티로 개발된 Kyverno와 같은 정책 엔진도 Admission Webhook을 사용합니다.\nReferences\n\nAdmission Controllers Reference\nA Guide to Kubernetes Admission Controllers\n"},"blog/CIS-Kubernetes-Benchmark":{"title":"CIS Kubernetes Benchmark","links":[],"tags":["Security"],"content":"\nCIS Benchmark란, 사이버 보안 향상을 위해 활동하는 국제 조직 Center of Internet Security에서 제안하는 IT 인프라 보안 관련 우수 사례 및 가이드라인을 의미합니다. 현재 다양한 종류의 CIS Benchmark가 존재하며, 이 중 Kubernetes와 관련된 것이 CIS Kubernetes Benchmark입니다.\nCIS Kubernetes Benchmark의 목적\n\nKubernetes에 적합한 보안 가이드라인을 기업과 조직에게 제공하여, 현재 알려져있는 Kubernetes 관련 취약점과 부적절한 설정(Misconfiguration)으로부터 안전한 Kubernetes 환경을 구축하는 것입니다.\n기업 내 엔지이너의 입장에선 일정한 툴을 사용하는 것만으로 Kubernetes 클러스터가 CIS Benchmark에서 권장하는 기준에 부합하는지 확인하고 적절한 조치를 취할 수 있게 됩니다.\n\nCIS Kubernetes Benchmark에서 다루는 범위\n\nKubernetes 클러스터 설정\nWorker Node 설정\nNetwork Policy\nRole-Based Access Control (RBAC)\nSecret 관리\nLogging 및 Monitoring\nPod 보안 정책\n컨테이너 보안\n\nCIS Kubernetes Benchmark의 구성 요소\nBenchmark는 다수의 권장 사항으로 구성되어 있습니다. 그리고 각 사항의 검토 결과는 아래 4가지 속성을 가지고 있습니다.\n\n\nScoring\n\nScored: 권장 사항이 Scored라면, 해당 권장 사항을 준수하지 못할 경우 최종 Benchmark 점수가 감소합니다.\nNot Scored: 권장 사항이 Not Scored라면, 해당 권장 사항을 준수하지 못해도 최종 Benchmark 점수는 감소하지 않습니다.\n\n\n\nLevels\n\nLevel 1: 명확한 보안 이점을 제공하거나, 기능성을 제한하지 않는 권장 사항이라는 뜻입니다.\nLevel 2: Level 1의 확장된 수준입니다. 심층적이거나 기능성에 부정적인 영향을 줄 수도 있는 권장 사항을 의미합니다.\n\n\n\nResults\n\nPass: 권장 사항을 준수했음을 의미합니다.\nFail: 권장 사항을 준수하지 못함을 의미합니다.\n\n\n\nResponsibility\n\n권장 사항을 준수해야 하는 주체를 의미합니다.\n주로 조직(Organization)으로 명시되지만, 특정 경우에는 서비스 제공자(vendor) 등이 명시되기도 합니다.\n\n\n\nCIS Kubernetes Benchmark 툴\n가장 보편적으로 쓰이는 툴은 kube-bench입니다.\n\nkube-bench는 해당 레파지토리에서 제공하는 yaml 파일을 받아온 뒤, 검토를 원하는 Kubernetes 클러스터 안에서 Job object로 실행하는 방식입니다.\n실행한 Job이 완료되면 해당 Pod의 로그로 Benchmark 검토 결과를 확인할 수 있습니다.\n\nReferences\n\nwww.armosec.io/glossary/cis-kubernetes-benchmark\nKubernetes CIS Benchmark: Why You Need It and Getting Started\n"},"blog/CKA-취득-후기":{"title":"CKA 취득 후기","links":[],"tags":["Certificate"],"content":"\n그동안 저는 Linux 재단에서 주관하는 Kubernetes 자격증을 꾸준히 취득하고 있었습니다. 그러다 마침내 저번주 KCSA 자격증 취득을 마지막으로, Linux 재단의 Kubernetes 자격증 5종을 모두 취득하게 되었는데요.\n이를 기념하고자 각 Kubernetes 자격증에 대한 소개와 저의 취득 후기를 시리즈로 구성하여 공유해보겠습니다.😃\n그 시작은 제가 가장 처음에 취득한 CKA(Certified Kubernetes Administrator)입니다.\n\nCKA 시험은 Kubernetes 관련 자격증 중 가장 많이 알려져있고 대중적입니다. CKA 자격증에 대한 정보는 아래와 같이 정리할 수 있습니다.\n\n응시료: $395\n\n코드를 입력하면 20% 또는 30% 할인해주는 쿠폰을 자주 배포하므로, 응시료 결제 전에 cka discount coupon을 찾아보는 것이 응시료를 절약할 수 있는 방법입니다.\n\n\n기본 언어: 영어\n\nLinux 재단 주관 Kubernetes 시험은 모두 기본 언어가 영어이므로, 시험 지문 역시 모두 영어인 점을 참고해야 합니다.\n\n\n시험 유형: 온라인 감독 환경에서 실습형\n응시 가능 기간: 결제 후 1년\n자격증 유효 기간: 합격 후 2년\n불합격 시 1회 재응시 기회 제공\n\nCKA에 응시한 이유\nKubernetes에 관심이 있어 공부를 하고 싶었는데, 어떻게 공부하면 좋을지 고민이 됐었습니다. 그러던 중에 무언가 남는 것이 있으면 좋겠다는 생각에 Kubernetes 자격증을 응시하기로 한 것이죠.\nKubernetes 자격증에 대해 조사해보니 CKA가 가장 많이 알려져있었고, Kubernetes 관리자(Administrator)를 위한 시험이기도 하니 Kubernetes에 대해 포괄적으로 다룰 수 있을 것 같아 응시하게 되었습니다.\nCKA 준비 방법\n가장 큰 도움을 받은 것은 Udemy의 Certified Kubernetes Administrator (CKA) with Practice Tests 강의입니다.\n\nKubernetes의 전반적인 이론과 개념을 알기 쉽게 설명해주는 강의였습니다.\n실습 과정이 포함되어 있어 Kubernetes의 기본기를 익히기에 좋았습니다.\n참고로 해당 강의는 영어로 촬영된 강의입니다. (Udemy의 기계 번역 자막이 지원되었던 걸로 기억합니다.)\n\nCKA 취득으로 얻은 것들\n그렇다면 저는 CKA를 취득하면서 어떤 것을 얻었을까요?\n가장 먼저, Kubernetes 기본 지식을 학습하면서 Kubernetes에 대한 관심이 더 깊어졌습니다. 이는 저에게 큰 영향을 주었고, 이후 제가 클라우드 엔지니어링 쪽으로 커리어를 쌓고자 한 계기가 되었습니다.\n그리고 CKA 자격증 취득 후, 제가 Kubernetes에 관심이 있고 꾸준히 공부하고 있음을 쉽게 알릴 수 있었습니다. 그리고 이러한 점은 이직 당시에도 유리하게 작용했다고 생각합니다."},"blog/CKS-취득-후기":{"title":"CKS 취득 후기","links":["📆-Project/Guide-to-DevOps/KO/CKA-취득-후기"],"tags":["Certificate"],"content":"\nCKS(Certified Kubernetes Security Specialist)는 클라우드 보안 지식을 중점적으로 확인하는 시험이며, 제가 두 번째로 취득한 Kubernetes 자격증입니다. CKS 자격증 취득을 계기로 저는 클라우드 보안과 클라우드 네이티브 프로젝트들에 대해 더 관심을 가지고 공부하게 되었죠.\n출제 범위를 제외한 CKS 자격증의 대한 응시 정보는 CKA와 거의 유사합니다.\n\n응시료: $395\n기본 언어: 영어\n시험 유형: 온라인 감독 환경에서 실습형\n응시 가능 기간: 결제 후 1년\n자격증 유효 기간: 합격 후 2년\n불합격 시 1회 재응시 기회 제공\n\nCKS에 응시한 이유\nCKS는 다른 Kubernetes 자격증과 달리 유일하게 응시 조건이 있었는데요. 바로 CKA 자격증을 먼저 취득해야 하는 것입니다. 그래서 CKS가 CKA의 상위 자격증이란 인식이 있었고, 실제로 난이도도 Kubernetes 자격증 중 가장 어렵다는 평이 있었기에, Kubernetes 역량을 더 키우고자 도전하게 되었습니다.\n또한 CKS의 출제 범위인 클라우드 보안에 대해서도 저는 거의 몰랐기 때문에 이 분야에 대해 공부하고 싶은 마음도 있었습니다.\nCKS 준비 방법\nUdemy 온라인 강의인 Kim Wüstkamp의 Kubernetes CKS Complete Course를 수강하면서 시험을 준비했었는데요. 앞서 말씀드렸던 것처럼, 저는 보안쪽 지식이 거의 없었기에 이 강의에 포함되었던 모든 이론 설명 영상과 실습 문제를 풀면서 개념을 익히고 시험을 준비했습니다.\n해당 강의를 모두 수강 후 시험에 합격할 수 있었으니, 클라우드 보안에 대한 개념을 잘 잡아주는 강의였다고 생각합니다.\n또한 강의 중간중간에 클라우드 보안과 관련된 좋은 블로그 글이나 컨퍼런스 발표 영상도 공유해줘서 관련 개념을 이해하는 데에 도움이 되었습니다.\n그리고 KodeKloud의 강의와 유사하게 Killer Shell이라는 플랫폼을 통해 클라우드 보안 관련 실습 문제를 풀 수 있었는데요. 역시 관련 개념을 잡기에 좋았습니다.\nCKS 취득으로 얻은 것들\nCKS를 취득하면서 클라우드 보안 분야에 대한 기본적인 지식을 얻었고, 해당 분야에 대해 더 깊이 관심을 가지게 되었습니다. 그동안 경험했던 DevOps 엔지니어링 분야와는 또다른 느낌이어서 신선하기도 했고, 보안도 소프트웨어 개발 및 운영에 있어 필수적인 것이라는 생각이 들어서요.\n그리고 CKS의 출제 범위에는 다양한 보안 관련 Third-party 툴들이 포함되는데요. 그 덕분에 이런 툴들을 대략적으로 경험할 수 있었고, 이로 인해 클라우드 네이티브 프로젝트들에 대해서도 관심을 가지게 되었습니다.\n마지막으로, 가장 난이도 높은 Kubernetes 자격증을 취득했다는 사실에 Kubernetes에 대한 자신감이 더 상승했습니다.😁\n물론 실무에서 활용할 수 있는 역량도 함께 길러야 하겠지만요.👍"},"blog/Clair-소개":{"title":"Clair 소개","links":["blog/컨테이너-보안-스캐닝"],"tags":["Security"],"content":"Clair는 컨테이너 이미지의 구성요소를 분석하여 취약점을 보고해주는 애플리케이션입니다. 이러한 활동을 컨테이너 보안 스캐닝이라고 하며, 더 자세한 설명은 여기서 확인하실 수 있습니다.\nClair의 동작 방식\nClair는 Indexer, Matcher, Notifier 컴포넌트로 구성되며, 취약점 분석은 Indexing과 Matching 순서로 진행됩니다.\n\n\n\nIndexing\n\n먼저 Clair의 Indexer가 스캔 대상인 컨테이너 이미지를 분석하여 이미지에 포함된 패키지, 이미지의 Base 이미지, 이미지에서 사용되는 레파지토리 등의 정보를 얻어냅니다.\n이렇게 얻어낸 컨테이너 이미지 관련 정보(Clair 공식 문서에선 Manifest라고 합니다.)는 DB에 저장되며, 해당 인덱싱에 대한 정보도 IndexReport라는 데이터 구조로 DB에 저장됩니다.\nIndexReport는 Matching 단계에서 취약점 분석에 쓰입니다.\n\n\n\nMatching\n\nClair의 Matcher가 DB에 저장된 IndexReport를 가져와 취약점 분석을 진행합니다.\n취약점 분석은 DB에 미리 저장되어있는 최신 취약점 데이터를 기반으로 진행됩니다.\nMatcher는 취약점 분석 후 VulnerabilityReport라는 취약점 보고서 객체를 생성합니다.\n취약점 보고서는 HTML 양식으로도 생성되어 웹 브라우저에서 조회 가능합니다.\n또한 Matcher는 주기적으로 최신 취약점 데이터를 DB에 업데이트하는 역할도 수행합니다.\n\n\n\nNotifications\n\nClair는 취약점 분석에 대한 알림 기능도 지원합니다.\nClair의 Notifier는 Matcher가 수행하는 DB의 최신 취약점 데이터 업데이트를 추적하고, 기존에 Indexer가 DB에 저장했던 컨테이너 이미지의 Manifest 중 취약점 데이터 업데이트에 영향을 받는 것이 있으면 알려주는 역할을 수행합니다.\nNotifier의 알림은 Webhook 등으로 받아볼 수 있습니다.\n\n\n\nClair의 활용 방안\n\nCI/CD에 Clair를 활용하면 서비스를 배포하기 전에 컨테이너 이미지를 스캔하여 보안 취약점을 사전에 최소화할 수 있습니다.\nClair의 Mathcer와 Notifier를 활용하여 최신 취약점 업데이트 및 알림을 팀내 전파하는 시스템을 구축하면, 개발자가 사전에 적절한 조치를 취할 수 있고 보안 이슈를 미리 예방할 수 있습니다.\n\nReferences\n\nwww.wiz.io/academy/container-security-scanning\nsnyk.io/learn/container-security/container-scanning\n"},"blog/Dockerfile의-ADD-vs-COPY":{"title":"Dockerfile의 ADD vs COPY","links":[],"tags":["Docker"],"content":"ADD 명령어와 COPY 명령어의 공통점\n두 명령어 모두 호스트의 특정 경로(출발 경로)에 있는 파일 또는 디렉토리를 Docker 이미지 안의 특정 경로(도착 경로)로 복사할 수 있습니다.\nADD 명령어의 또다른 기능들\nCOPY 명령어와 달리, ADD 명령어에는 일반 복사 외에 추가로 지원하는 기능이 있습니다.\nURL을 활용하여 파일 다운로드\n\nADD 명령어의 출발 경로를 입력하는 부분에 호스트 경로 대신 URL을 입력할 수 있습니다.\nURL을 입력할 경우, 해당 원격지로부터 파일을 다운로드하여 Docker 이미지의 도작 경로에 추가됩니다.\n\n압축 자동 해제 및 추출\n\nADD 명령어의 출발 경로에 호스트 내에 압축(gz, bz2, xz)된 tar 아카이브 파일이 들어갈 수도 있습니다.\n압축 파일은 자동으로 해제되며, 이때 추출된 디렉토리가 Docker 이미지의 도착 경로에 저장됩니다.\n"},"blog/Dockerfile의-RUN,-CMD,-ENTRYPOINT-명령어":{"title":"Dockerfile의 RUN, CMD, ENTRYPOINT 명령어","links":[],"tags":["Docker"],"content":"Dockerfile을 통해 Docker 컨테이너를 실행하는 과정에서, 특정한 작업이나 애플리케이션의 프로세스를 실행시킬 때 RUN, CMD, ENTRYPOINT 명령어를 사용합니다. 각 명령어에 어떤 차이점이 있는지 알아보겠습니다.\nRUN 명령어\n먼저 RUN 명령어는 Dockerfile에서 이미지를 생성할 때 가장 많이 쓰이는 명령어로, 애플리케이션이나 패키지를 설치할 때 사용합니다. 기존 이미지 위에 새로운 레이어를 생성하는 명령어입니다.\nFROM ubuntu:18.04\nRUN apt-get update\nRUN apt-get install -y nginx\n위 Dockerfile 예제에서는 Ubuntu 18.04 환경 위에 apt-get update 명령어로 패키지 업데이트를 한 다음, nginx를 설치합니다. 다만 위와 같은 Docker 이미지는 패키지 업데이트 및 애플리케이션 설치만 될 뿐, 어떠한 프로세스가 실행되지 않기 때문에 컨테이너가 정상적으로 동작하지 않습니다.\n정상적으로 동작하는 Docker 이미지를 만들기 위해서는 컨테이너 내부에서 최종적으로 실행되는 프로세스를 CMD 또는 ENTRYPOINT 명령어로 정의해야 합니다.\nCMD 명령어\n컨테이너 내부에서 최종적으로 실행되는 프로세스의 명령어로, Docker CLI상에서 다른 명령어로 덮어씌울 수 있습니다.\nFROM ubuntu:18.04\nRUN apt-get update\nCMD [&quot;echo&quot;, &quot;Hello, World&quot;]\n위 Dockerfile 예제를 이용하여 Docker 이미지를 빌드하고, docker run [이미지 이름] 명령어로 실행 시 실행 결과는 아래와 같이 CMD 명령어로 정의한 프로세스가 실행됩니다.\nHello, World\n\n만약 이미지 빌드 후 docker run [이미지 이름] hostname 명령어로 실행할 경우, CMD 명령어로 정의한 프로세스가 아닌 hostname 프로세스만 실행되어 컨테이너의 호스트네임이 표시됩니다.\n4aa43esdd984\n\nENTRYPOINT 명령어\nCMD 명령어와 달리, Docker CLI상에서 다른 명령어로 덮어씌울 수 없습니다.\nFROM ubuntu:18.04\nRUN apt-get update\nENTRYPOINT [&quot;echo&quot;, &quot;Hello, World&quot;]\n위 Dockerfile 예제를 이용하여 Docker 이미지를 빌드하고, docker run [이미지 이름] 명령어로 실행 시 실행 결과는 아래와 같이 ENTRYPOINT 명령어로 정의한 프로세스가 실행됩니다.\nHello, World\n\n만약 이미지 빌드 후 docker run [이미지 이름] hostname 명령어로 실행할 경우, ENTRYPOINT 명령어로 정의한 echo Hello, World 뒤에 hostname 명령어가 따라 붙어 함께 실행됩니다.\nHello, World 4aa43esdd984\n"},"blog/Docker에서-사용하는-네트워크":{"title":"Docker에서 사용하는 네트워크","links":[],"tags":["Docker","Network"],"content":"Docker 네트워크의 종류\nDocker에서 사용하는 네트워크 중 대표적인 것은 아래 3가지입니다.\n\nBridge\nHost\nOverlay\n\nBridge 네트워크\n\n가상 인터페이스와 호스트의 인터페이스를 연결하여 도커 컨테이너가 외부와 연결 가능하게 해주는 네트워크입니다.\nDocker가 실행될 때 자동으로 Bridge 네트워크가 생성되며, Docker 컨테이너 생성 시 네트워크를 따로 지정하지 않으면 Docker 컨테이너가 미리 생성되어 있던 Bridge 네트워크와 연결됩니다.\n\nHost 네트워크\n\n호스트 네트워크 환경과 IP를 그대로 사용하며, Docker 컨테이너 내부 애플리케이션을 별도로 포트포워딩하지 않고 외부에서 접근 가능합니다.\n\nOverlay 네트워크\n\n다른 호스트 간에 네트워크를 공유하는 방식입니다. (호스트가 여러 개일 때 사용)\nOverlay 네트워크에 연결된 컨테이너들은 암호화 방식을 사용하여 서로 안전하게 통신할 수 있습니다.\n"},"blog/Docker에서-사용하는-파일-시스템":{"title":"Docker에서 사용하는 파일 시스템","links":[],"tags":["FileSystem","Docker"],"content":"UFS(Union File System)\n\nUFS는 여러 개의 파일 시스템을 하나의 파일 시스템에 마운트하는 방식입니다.\n현재 Docker에서는 UFS를 구현하기 위한 Storage Driver로 Overlay를 주류로 사용합니다.\nDocker 이미지에서 Layer는 각각의 파일 시스템을 겹쳐 놓은 형태와 유사합니다.\nLayer는 Container Layer와 Image Layer로 나뉩니다.\n\nContainer Layer:\n\n쓰기 작업이 가능한 Layer입니다.\n각 컨테이너의 최상단 Layer이며, 컨테이너 생성 후 모든 변경 작업이 이루어지는 Layer입니다.\nR/W 속도는 상대적으로 느립니다.\n\nImage Layer:\n\n읽기 작업만 가능한 Layer입니다.\n다른 컨테이너와 공유되는 Layer입니다.\n"},"blog/Falco-소개":{"title":"Falco 소개","links":["blog/컨테이너-런타임-보안"],"tags":["Security"],"content":"\nFalco는 Linux 시스템을 대상으로 개발된 클라우드 네이티브 보안(Cloud-native Security) 툴입니다. Linux 커널 이벤트에 대한 규칙(Rule)을 지정하면 이에 따른 실시간 알림을 보내주는 역할을 수행합니다.\nFalco가 지원하는 이러한 행위를 컨테이너 런타임 보안 활동이라고 하는데요. 이와 관련된 자세한 설명은 여기서 확인하실 수 있습니다.\nFalco를 사용하는 이유\n현재 컨테이너 런타임 보안 활동을 지원하기 위해 다양한 툴이 개발 및 유지보수되고 있습니다. 그 중 Falco는 아래와 같은 장점을 지니고 있습니다.\n\nKubernetes, Docker Swarm과 같은 다양한 Container Orchestration Platform에서 사용 가능\nKubernetes의 Daemonset으로 배포 및 실행을 지원하므로 모든 노드에 대한 모니터링이 용이함\nKubernetes API Call을 활용할 수 있어 클러스터 내 Node 및 Pod의 상태를 모니터링 가능\nFalco 가 사용할 수 있는 Default Ruleset이 미리 정의되어 있어 컨테이너 런타임 보안 활동을 쉽고 빠르게 시작 가능\n특정 조건에 대한 이상 행위를 감지하는 Custom Ruleset을 쉽게 정의 가능\n\nFalco의 동작 방식\nFalco의 동작 프로세스는 크게 아래 3단계로 나눌 수 있습니다.\n\nLinux Kernel의 System Call 또는 Kunernetes API Call 포착\n사전 정의된 Ruleset (또는 Default Ruleset)에 따라 표시할 Output 처리\n사전 정의된 채널로 Output 전송\n\nFalco는 동작 중인 컨테이너에서 생성되는 여러 요청을 감시하기 위해 Ruleset을 사용하는데요. Falco의 Ruleset은 아래와 같이 정의됩니다.\n- rule: Detect bash in a container # Rule 이름\n  desc: You shouldn&#039;t have a shell run in a container # Rule 설명\n  condition: container.id != host and proc.name = bash # 프로세스의 Call 중 Falco가 포착하는 조건\n  output: Bash ran inside a container (user=%user.name command=%proc.cmdline %container.info) # 해당 Rule에 대해 Falco가 표시하는 Output 양식\n  priority: INFO # Falco가 표시하는 Output의 Type\nFalco는 다양한 채널을 통해 Output을 전송합니다. 그 중 대표적인 채널은 아래와 같습니다.\n\nStandard Output\nFile\nSyslog\nHTTP[s]\ngRPC\n\nFalco 설치 방식\n\n\nKubernetes 클러스터에 Helm Chart로 설치\n\nKubernetes 클러스터에 Falco를 가장 쉽고 빠르게 설치하는 방법은 Helm을 이용하는 것입니다.\nHelm으로 Falco를 설치하면, 배포된 Falco Pod의 로그를 확인함으로써 Falco Ouput 모니터링이 가능합니다.\nCustom Ruleset을 yaml 파일로 정의한 다음, Helm으로 배포되어있는 Falco에 적용하는 것도 가능합니다.\n\n\n\nLinux 호스트에 패키지로 설치\n\nFalco 공식 패키지 저장소에서 각 시스템 아키텍처에 맞는 버전을 직접 설치 가능합니다.\n이렇게 설치한 Falco의 Configuration 및 Ruleset 파일 역시 호스트에 저장 및 사용되므로 수정이 필요하다면 호스트 내 파일을 직접 수정해야 합니다.\n\n\n\nLinux 호스트에 컨테이너 이미지로 배포\n\nFalco의 공식 컨테이너 이미지 저장소에서 Falco 실행이 가능한 컨테이너 이미지를 받아와 호스트에 배포할 수도 있습니다.\n\n\n\nReferences\n\nfalco.org/\ngithub.com/falcosecurity/charts/tree/master/charts/falco\nsysdig.com/blog/intro-runtime-security-falco/\n"},"blog/Fluent-Bit에서-발견된-보안-취약점-(CVE-2024-4323)":{"title":"Fluent Bit에서 발견된 보안 취약점 (CVE-2024-4323)","links":[],"tags":["CVE","Monitoring"],"content":"\n다양한 클라우드 제공자와 IT 기업에서 널리 사용 중인 로그 데이터 수집 툴 Fluent Bit에서 보안 취약점(CVE-2024-4323)이 발견되었습니다.\n해당 취약점은 Fluent Bit의 내장 HTTP 서버에 존재하던 버퍼 오버플로(Buffer Overflow)가 원인이라고 합니다.\n해당 취약점이 위험한 이유\n이번 취약점에 대한 테스트 결과, Buffer Overflow 동작으로 인해 인접한 메모리에 저장되어있던 정보가 HTTP 응답으로 반환되는 경우가 있었다고 합니다. 이는 민감한 정보가 유출될 위험이 있음을 보여주는데요.\n그 외에도 서비스 거부(Denial of Service), 원격 악성 코드 실행 공격 등에 해당 취약점이 이용될 수도 있다고 합니다.\n다만 해당 취약점을 이용해서 원격 악성 코드 실행을 수행하는 것은 비교적 어렵기 때문에, 해당 공격 위험이 급박한 것은 아니라고 합니다.\n대처 방법\n해당 보안 취약점에 대처할 수 있는 가장 확실한 방법은, Fluent Bit를 v3.0.4 이상으로 업그레이드하는 것입니다.\n만약 버전을 바로 업그레이드하기 어렵다면, 인증된 사용자와 서비스만 Fluent Bit의 모니터링 관련 API에 접근할 수 있도록 설정하는 것이 권장된다고 합니다.\nReferences\n\nCritical Fluent Bit flaw affects major cloud platforms, tech companies’ offerings (CVE-2024-4323)\nwww.tenable.com/plugins/nessus/197568\n"},"blog/Istio-소개":{"title":"Istio 소개","links":["blog/Service-Mesh-소개"],"tags":["Network"],"content":"\nIstio는 클라우드 환경에서 Service Mesh를 구현할 수 있는 오픈소스 솔루션입니다. Service Mesh가 무엇이고 왜 사용되는지 궁금하시다면, 여기를 확인해보세요.\nIstio를 사용하는 이유\nIstio는 Service Mesh의 주요 기능들을 모두 지원하고 있는데요. Istio를 사용함으로써 얻을 수 있는 이점을 각 측면 별로 정리하면 아래와 같습니다.\n\n가시성\n\nService Mesh에서 발생하는 모든 Service 통신 관련 로그와 메트릭을 쉽게 모을 수 있음\n\n\n트래픽 관리\n\nService 사이의 트래픽과 API 요청을 별도의 레이어에서 제어 가능\nCircuit Breaker, Timeout, Retry와 같은 Service에 대한 속성을 별도 영역에서 쉽게 설정 가능\n\n\n보안성\n\n트래픽 허용 관련 Policy나 TLS 암호화, 인증 등의 기능 제공\n\n\n도입 용이성\n\nIstio 생태계의 규모가 큰 덕분에 다양한 기술 문서나 적용 사례를 참고 가능\n\n\n\nIstio의 동작 방식\nIstio는 Service Mesh와 마찬가지로 데이터 플레인(Data Plane)과 컨트롤 플레인(Control Plane)으로 구성됩니다.\n\n데이터 플레인\n\nKubernetes의 Service 간의 통신을 담당\nEnvoy라는 오픈소스 프록시를 각 Pod 내에 Sidecar로 배포\nEnvoy 프록시가 Service에 대해 발생하는 네트워크 트래픽을 가져와서 Istio의 각 기능을 수행\nEnvoy 프록시는 컨트롤 플레인에서 Configuration한 설정이 일괄 적용됨\n\n\n컨트롤 플레인\n\nService에 추가되는 프록시에 대한 Configuration을 담당하는 영역\n\n\n\nReferences\n\nistio.io/latest/about/service-mesh\n"},"blog/Jenkins와-Pipeline-(Declarative-vs-Scripted)":{"title":"Jenkins와 Pipeline (Declarative vs Scripted)","links":[],"tags":["Jenkins"],"content":"Jenkins와 Pipeline\nJenkins는 워크 플로우를 자동화할 수 있는 오픈 소스 툴입니다. 소프트웨어의 지속적 통합(Continuous Integration, CI)과 지속적 배포(Continuous Delivery, CD)를 간단하게 구현할 수 있도록 도와주죠. 특히 Jenkins의 Pipeline을 사용하면 배포 과정을 더욱 쉽고 빠르게 구축할 수 있기 때문에, CI/CD 프로세스를 구현할 때 Jenkins를 사용하는 경우가 많습니다.\n지속적 배포(CD) 파이프라인을 예로 들어 Jenkins의 Pipeline에 대해 더 살펴보겠습니다. CD 파이프라인은 소프트웨어를 버전 관리 시스템(예: GitHub)에서부터 사용자 및 클라이언트에게로까지 바로 전달하는 일련의 프로세스라고 할 수 있습니다. 소프트웨어의 새로운 버전을 릴리즈하는 데에 필요한 모든 워크 프로우, 활동, 자동화 과정이 여기에 포함되며, 이러한 과정을 Jenkins의 Pipeline 파일의 코드로 구현할 수 있는 것입니다.\nJenkins Pipeline 코드를 작성하는 방식에 따라 Declarative Pipeline과 Scripted Pipeline으로 나뉩니다.\nDeclarative Pipeline\nDeclarative 방식은 Scripted 방식보다 최근에 Jenkins에 추가되었으며, Pipeline 작성에 특화된 문법을 따르기 때문에 코드 작성과 관리가 더욱 용이해졌습니다.\n코드 예제\npipeline {\n  agent any\n  stages {\n    stage(&#039;Hello World) {\n      steps {\n        sh &#039;echo Hello World&#039;\n      }\n    }\n  }\n}\n장점\n\n보다 구조화된 Pipeline 코드 작성이 가능하여 코드 관리 용이성과 가독성을 높일 수 있습니다.\nBlue Ocean 인터페이스와 연동하기 쉽습니다.\n\n단점\n\nScripted 방식보다 코드 작성의 자유도가 떨어지고, 복잡한 로직의 Pipeline 코드를 작성하기 어려울 수도 있습니다.\nDeclarative 방식을 지원하지 않는 플러그인도 존재합니다.\n\nScripted Pipeline\nScripted 방식은 Jenkins가 기존에 지원하던 Pipeline 작성 방식입니다. Groovy 언어가 제공하는 대부분의 기능을 사용할 수 있어 Pipeline 코드를 유연하게 작성 가능합니다.\n코드 예제\nnode {\n  stage(&#039;Hello World) {\n    sh &#039;echo Hello World&#039;\n  }\n}\n장점\n\nPipeline 코드 작성이 비교적 자유롭습니다.\n복잡한 로직의 Pipeline 코드 작성도 가능합니다.\n\n단점\n\n구조화된 양식이 없기 때문에 불안정한 Pipeline 코드를 작성할 위험이 있으며, 코드를 이해하거나 관리하기 어려워질 수도 있습니다.\n"},"blog/LLMOps에-대해-알아보기":{"title":"LLMOps에 대해 알아보기","links":[],"tags":["LLMOps"],"content":"요즘 ChatGPT의 인기 덕분에 LLM(Large Language Model)이 큰 관심을 받고 있습니다.\n혹시 LLMOps라는 컨셉에 대해 들어보셨나요?\n이미 많이 알고계신 DevOps(Development + Operations)와 같은 원리로 LLM과 Operations가 합쳐진 용어인데요.\nLLMOps는 LLM의 전체 라이프 사이클 주기 동안 모델을 효율적으로 개발, 배포, 관리할 수 있도록 특화된 방법론 및 프로세스를 의미합니다.\nLLMOps가 필요한 이유\n그렇다면 LLMOps가 왜 필요할까요? 가장 큰 이유는 보안 때문입니다.\n어떤 기업의 직원이 외부의 LLM 제공업체의 서비스를 이용한다고 가정해보겠습니다. 이 직원은 자신의 업무 수행에 도움을 받기 위해 외부 LLM 서비스에 이것저것 질문을 할 텐데요.\n이때 회사 내부 정보도 질문에 포함될 가능성은 충분히 있겠죠. 게다가 이런 질문들은 LLM 제공업체의 서버로 전송되기 때문에, 기업 입장에선 외부 LLM 서비스를 이용하기 어렵습니다.\n이런 이유로 기업 내부에서 자체 LLM 서비스를 운영 및 관리하는 경우가 생기기 시작합니다. 그리고 효율적이면서 고도화된 LLM 학습, 운영, 관리를 위해 LLMOps가 필요하게 되는 것이죠.\nLLMOps의 동작 방식\nLLMOps 주기는 아래와 같이 나눌 수 있습니다.\n\nFM(Foundation Model) 선정\nUse Case에 맞춰 적용\n평가(Evaluation)\n배포\n모니터링\n\n그리고 LLMOps의 Workflow를 도식화하면 아래 그림처럼 표현할 수 있죠.\n\nMLOps와 LLMOps의 차이점\nLLMOps가 나오기 전에 이미 MLOps라는 용어가 있었습니다. 머신러닝 모델의 학습, 개발, 관리를 위해 생겨난 개념인데요.\n두 개념 모두 AI 모델의 전체 개발 주기를 대상으로 한다는 공통점이 있지만, 차이점도 있습니다. LLMOps를 좀 더 잘 이해할 수 있도록 MLOps와의 차이점을 살펴보겠습니다.\n먼저 LLMOps에서는 FM(Foundation Model)을 사용합니다. 대규모 학습이 필요한 LLM의 특성상, AI 모델을 처음부터 새로 만들고 학습시키려면 엄청난 규모의 자원이 소모되기 때문입니다.\n\nFM은 기반 모델이란 뜻으로, 다양한 도메인의 대규모 데이터셋으로 미리 학습된 모델을 의미합니다.\n참고로 이렇게 FM을 기반으로 AI 모델을 학습/운영하는 접근법은 FMOps라고 하며, 현재 FMOps와 LLMOps라는 용어는 엄격히 구분하지 않고 혼용됩니다.\n\n또한 LLMOps 관점에선 사용자의 피드백이 성능 개선에 중요하므로, 피드백 데이터가 모델에 반영될 수 있는 파이프라인 설계가 추가로 필요하다는 차이점도 있습니다.\nReferences\n\nLLMOps가 주목받고 있는 이유\nwww.ibm.com/topics/llmops\nubiops.com/llmops-vs-mlops\nFMOps and LLMOps: Operationalize Generative AI at Scale\n"},"blog/Liveness,-Readiness,-Startup-Probe-소개-및-비교":{"title":"Liveness, Readiness, Startup Probe 비교","links":[],"tags":["Kubernetes"],"content":"🔎Kubernetes의 Container Probe란?\n\nKubernetes에서 Pod를 배포하면 Pod에서 정의한 컨테이너가 실행되는데요. 컨테이너 내부에서 실행되어야 하는 프로세스가 정상 작동할 때 비로소 Pod를 통해 원하는 서비스를 이용할 수 있게 됩니다.\nPod를 배포하고 운영하다보면 동작 중이던 컨테이너의 상태가 정상인지 주기적으로 확인이 필요할 때가 있습니다. 혹은 해당 컨테이너가 외부 트래픽을 받을 준비가 되었는지 알아야 할 때도 있죠.\n이렇게 컨테이너의 상태를 주기적으로 진단할 때 사용하는 것이 바로, Container Probe입니다.\n🩺Container Probe의 진단 유형과 Probe의 종류\nKubernetes의 Container Probe는 Manifest의 컨테이너 레벨에서 정의되는데요. 정의된 진단 설정에 따라 kubelet이 해당 컨테이너 내부에서 주기적으로 진단을 수행합니다.\nProbe는 아래와 같이 4가지 방법 중 하나로 컨테이너의 상태를 진단할 수 있습니다.\n\nexec\n\n컨테이너 내부에서 실행할 명령어 지정\n명령어 실행 후 상태 코드가 0이면 성공으로 진단\nexec는 수행될 때마다 컨테이너 내에 새로운 프로세스가 생성되기 때문에 Node의 CPU 사용량이 증가할 수 있으므로 주의가 필요\n\n\ngrpc\n\ngRPC 요청으로 진단하며, gRPC Health Check이 미리 구현되어 있어야 함\n응답 status가 SERVING이면 성공으로 진단\n\n\nhttpGet\n\n특정 port 및 path로 Pod의 IP에 대해 HTTP GET 요청으로 진단\n응답 status code가 200 이상 400 미만이면 성공으로 진단\n\n\ntcpSocket\n\n특정 port로 Pod의 IP에 대해 TCP 요청으로 진단\n해당 port가 열려 있으면 성공으로 진단\n\n\n\n이렇게 수행한 진단 결과는 아래 3가지 중 하나로 나옵니다.\n\nSuccess: 진단 성공\nFailure: 진단 실패\nUnknown: 진단 실패, Faliure와 달리 kubelet이 추가 진단 실행\n\nkubelet이 수행하는 Probe는 아래와 같이 3가지 유형으로 나뉘는데요.\n\nReadiness Probe\nLiveness Probe\nStartup Probe\n\n각 Probe에 대해 아래에서 더 자세히 알아보겠습니다.\n✨Readiness, Liveness, Startup Probe\nReadiness Probe\nReadiness Probe는 트래픽과 관련이 있는데요. Readiness Probe를 사용하면 해당 컨테이너로 트래픽이 들어오는 시점을 제어할 수 있기 때문입니다.\n즉, 컨테이너의 상태 진단 결과가 성공인 시점부터 해당 컨테이너에 트래픽이 들어오는 것을 허용하고 싶을 때 Readiness Probe를 사용하는 것이죠.\n각 Probe는 컨테이너 레벨에서 정의한다고 했는데요. 아래 Pod Manifest 예제로 Probe를 어떻게 정의하는지 알아보겠습니다.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: goproxy\n  labels:\n    app: goproxy\nspec:\n  containers:\n  - name: goproxy\n    image: registry.k8s.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:  # Readiness Probe를 정의하는 부분입니다.\n      httpGet:\n\t    path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 10\n위 예제에서 Readiness Probe를 정의하는 부분만 살펴보면 해당 Probe의 작동 방식은 아래와 같습니다.\n\n위 Readiness Probe는 HTTP GET 요청으로 진단 수행 (httpGet)\nHTTP GET 요청을 보내는 Port와 경로는 8080 및  /healthz\nkubelet이 첫 Probe 진단을 수행되는 시점은 컨테이너 실행 후 15초 뒤 (initialDelaySeconds)\nkubelet은 10초 간격으로 Probe 진단 수행 (periodSeconds)\n\n위 예제에선 컨테이너에 readinessProbe 하나만 정의되었지만, 필요에 따라 컨테이너에 대해 Probe를 종류별로 모두 정의할 수도 있습니다.\nLiveness Probe\nLiveness Probe는 동작 중인 컨테이너의 상태를 주기적으 진단할 때 사용됩니다.\nKubernetes에 배포된 컨테이너의 프로세스에 이슈가 생기거나 프로세스의 상태가 Unhealthy인 경우, kubelet이 이를 감지하여 자동으로 Pod의 restartPolicy(재시작 정책)에 따라 조치를 취하기 때문에 Liveness Probe가 반드시 필요한 것은 아닙니다.\n하지만 Probe의 컨테이너 상태 진단 결과에 따라 종료 또는 재시작이 필요한 경우라면 Liveness Probe를 사용할 수 있습니다.\n위 예제 코드에서 봤던 것처럼, 모든 Probe는 spec.containers 레벨에서 정의할 수 있는데요. 위 코드에 exec를 수행하는 Liveness Probe를 추가로 정의한다면 아래와 같겠습니다.\nspec:\n  containers:\n  - name: goproxy\n    image: registry.k8s.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:  # Readiness Probe를 정의하는 부분입니다.\n      httpGet:\n\t    path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 10\n    livenessProbe:  # 동일한 컨테이너에 대해 Liveness Probe를 추가로 정의했습니다.\n      exec:\n        command:\n\t\t- cat\n\t\t- /tmp/healthy\n      initialDelaySeconds: 5\n      periodSeconds: 5\n위 예제처럼 Liveness Probe와 Readiness Probe를 동일한 컨테이너에 대해 함께 사용할 경우, 아래와 같이 각 Probe의 역할 수행을 기대할 수 있습니다.\n\nReadiness Probe로 해당 컨테이너가 준비될 때까지 들어오는 트래픽 제어\nLiveness Probe로 해당 컨테이너 동작 중에 진단 실패 시 자동으로 재시작\n\nStartup Probe\n마지막으로 Startup Probe는, 그 이름처럼 컨테이너가 정상적으로 시작했는지 여부를 진단합니다.\n가동하는 데에 시간이 오래 걸리는 컨테이너의 상태를 진단해야 하는 경우라면, Liveness Probe의 periodSeconds 값을 길게 설정해서 진단 주기를 넓히기보다는 Startup Probe를 정의해서 컨테이너의 시작 성공 여부를 파악하는 것이 좋은데요.\n그래야 Liveness Probe로는 동작 중인 컨테이너의 상태를 적절한 주기로 진단할 수 있기 때문입니다.\nStartup Probe는 아래와 같이 정의할 수 있습니다.\nstartupProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  failureThreshold: 30\n  periodSeconds: 10\n위 예제를 보면 failureThreshold(최대 실패 허용 횟수) 값이 설정되어 있는데요. 해당 Startup Probe는 최대 300초(30 * 10) 동안 주기적으로 컨테이너가 정상적으로 실행했는지를 진단하고, 300초가 지나도록 진단에 실패하면 Pod의 restartPolicy(재시작 정책)에 따라 컨테이너를 재시작하거나 종료하게 됩니다.\n한 가지 참고할 점은, Startup Probe의 진단이 성공하기 전까진 Readiness Probe와 Liveness Probe가 실행되지 않다는 것입니다.\n이렇게 각 Probe는 수행 시점과 역할이 조금씩 다르기 때문에, 컨테이너의 성격이나 운영 정책에 따라 적절한 조합으로 Probe를 사용한다면 더욱 효과적으로 Pod를 배포하고 운영할 수 있습니다.\nReferences\n\n# Configure Liveness, Readiness and Startup Probes\n# Pod Lifecycle\n"},"blog/Service-Mesh-소개":{"title":"Service Mesh 소개","links":["blog/Istio-소개"],"tags":["Network"],"content":"\n클라우드 개발 환경에서 종종 Service Mesh란 키워드를 들으신 적이 있을 수 있습니다. Service Mesh라고 하면 네트워크와 관련된 것 같은 느낌이 있는데요.\n이름에 담긴 느낌처럼, Service Mesh의 간략한 정의는 ‘애플리케이션의 서비스 간 모든 통신을 처리하는 소프트웨어 계층’이라고 할 수 있습니다.\nService Mesh를 도입하는 이유\n그렇다면 Service Mesh를 도입해서 서비스 간 모든 통신을 처리하고 관리해야 할 이유는 무엇일까요? 이는 아래와 같이 정리할 수 있습니다.\n\n가시성\n\nMSA 환경에서 서비스 간에 주고받는 트래픽에 대한 로그를 얻을 수 있으므로, 애플리케이션 외부에서 일어나는 이벤트의 가시성을 확보 가능\n\n\n트래픽 관리\n\n서비스 간에 주고받는 트래픽을 독립된 레이어에서 제어 가능\n\n\n보안성\n\n상호 TLS(mTLS) 설정으로 서비스간 통신에서 검증이 가능\n별도의 Policy를 적용하여 서비스간 통신 가능 여부를 제어할 수 있고, 이를 코드로 관리 가능\n\n\n\nService Mesh가 동작하는 방식\n그렇다면 Service Mesh는 어떻게 동작할까요? Service Mesh의 두 구성 요소인 데이터 플레인(Data Plane)과 컨트롤 플레인(Control Plane)으로 나눠서 알아보겠습니다.\n\n데이터 플레인\n\nService Mesh의 데이터 처리 역할 수행\n각 서비스에 Service Mesh를 위한 별도의 네트워크 프록시가 Sidecar로 추가되며, 해당 프록시는 서비스로 들어오고 나가는 모든 트래픽이 거치는 중간 게이트 역할\n서비스 간의 통신이 발생할 때 동작 과정\n\nSidecar로 추가되었던 네트워크 프록시가 요청을 받음\n받은 요청을 별도의 네트워크 연결로 캡슐화\n출발 프록시와 도착 프록시 간의 안전하고 암호화된 채널 설정\n\n\n이외에도 로드 밸런싱, Circuit Breaker, 트래픽 라우팅과 같은 역할도 수행\n\n\n컨트롤 플레인\n\nService Mesh의 중앙 관리 및 구성 계층 역할 수행\n서비스 간의 라우팅 규칙, 로드 밸런싱 정책, 보안 설정 등이 가능한 영역\n이외에도 Mesh 내 모든 서비스를 추적하는 레지스트리, Telemetry 데이터의 수집 및 집계 역할도 수행\n\n\n\nService Mesh를 구현하는 방법\n지금까지 알아본 Service Mesh는 어떻게 적용할 수 있을까요? 대표적인 Service Mesh 솔루션으로 Istio가 있습니다.\nIstio는 Kubernetes와 함께 작동하도록 설계되어 있어 호환성이 높으며, 이때 Istio의 네트워크 프록시가 Kubernetes의 Pod 내 Sidecar 컨테이너로 추가되는 방식으로 Service Mesh를 구현합니다.\nIstio와 관련된 더욱 자세한 글은 여기서 확인해보세요.\nReferences\n\nIstio는 무엇이고 왜 중요할까?\n서비스 메시란 무엇인가요?\n실무자를 위한 서비스 메시 - 지금 서비스 메시가 의미 있는 이유\n"},"blog/kube-bench-소개":{"title":"kube-bench 소개","links":["blog/CIS-Kubernetes-Benchmark"],"tags":["Kubernetes","Security"],"content":"kube-branch는 CIS Kubernetes Benchmark를 기반으로 k8s 환경이 보안적으로 안전한지 검토해주는 툴입니다.\nCIS Kubernetes Benchmark에 대해 궁금하시다면 여기서 확인해보세요.\nkube-bench를 사용하는 이유\nkube-bench는 공식 레파지토리에서 배포하는 Job yaml 파일 또는 Docker 이미지를 실행하는 것만으로도 CIS Kubernetes Benchmark 검토를 쉽게 수행할 수 있고, Benchmark 결과도 바로 확인할 수 있기 때문에 많이 사용됩니다.\nkube-bench 실행 방법\n컨테이너 이미지로 실행\n\nk8s 환경에서 아래 명령어로 kube-bench의 Docker 이미지를 실행합니다.\n\ndocker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t docker.io/aquasec/kube-bench:latest --version 1.28\n이때 --version은 현재 검토하려는 k8s 클러스터의 버전을 명시합니다.\n\n\n명령어를 실행하면 kube-bench의 공식 Docker 이미지를 통해 CIS Benchmark 검토가 진행되며, 완료 시 검토 결과를 바로 확인할 수 있습니다.\n\nk8s Job Object로 실행\n\nk8s 환경에서 kube-bench 공식 레파지토리에서 제공하는 Job yaml 파일을 가져옵니다. (파일 링크)\n가져온 yaml 파일을 이용해 kube-bench를 실행하는 k8s Job을 아래 명령어로 클러스터에 배포합니다.\n\nkubectl apply -f {가져온 yaml 파일 이름}.yaml\n\n\n배포한 Job 실행이 완료되었다면 아래 명령어로 Benchmark 검토 결과를 확인할 수 있습니다.\n\nkubectl logs {배포한 Job의 Pod 이름}\n\n\n\nkube-bench의 실행 결과\n\nkube-bench를 원하는 방법으로 실행하고나면, 위 이미지와 같은 검토 결과를 얻을 수 있습니다.\n각 카테고리 별로 CIS 보안 사항에 대한 검토 결과를 확인할 수 있으며, 현재 보안 사항이 준수되고 있는지의 여부(PASS/FAIL)와 보안 사항의 세부 설명에 대해서도 확인 가능합니다.\nReferences\n\ngithub.com/aquasecurity/kube-bench\n"},"blog/ollama와-Open-WebUI-로컬-배포":{"title":"ollama와 Open-WebUI 로컬 배포","links":[],"tags":["Docker","Ollama"],"content":"ollama와 Open-WebUI\nLLM을 활용한 서비스가 다양하게 출시되는 요즘, 로컬에서 LLM을 사용할 수 있도록 도와주는 ollama이라는 툴에 관심이 생겼습니다.\n오픈 LLM 모델의 GGUF 파일이 있다면 ollama를 이용해 로컬 환경에서 LLM과 상호작용이 가능한데요.\n로컬 LLM은 개인 정보 유출 위험이 적고 비용 발생도 없다는 장점이 있습니다.\n그래서 제 노트북의 로컬 환경에 직접 ollama를 실행시킨 다음, 공개된 LLM 모델을 가져와 테스트를 진행해봤습니다.\n테스트를 진행한 노트북 사양은 아래와 같습니다.\n\nCPU: AMD Ryzen 7 4800H with Radeon Graphics 2.90 GHz\nRAM: 32 GB\n\nCLI 환경에서 동작하는 ollama를 보다 쉽게 사용하기 위해, Open-WebUI라는 툴을 함께 사용했는데요.\nOpen-WebUI는 Chat GPT와 유사한 UI를 가지고 있고, 호스트에 실행 중인 ollama와 연동되어 웹 브라우저상에서 LLM에 질문을 하거나 다양한 LLM 관련 설정도 가능합니다.\nDocker Compose를 활용하여 로컬 배포\nollama와 Open-WebUI 로컬 배포에 대해 조사해보니 모두 로컬에 직접 설치하는 글이 대부분이었지만, 각 툴이 호스트 환경으로부터 독립되어야 일관된 기능이 보장될 수 있으므로 우리는 Container 환경에서 실행해보도록 하겠습니다.\n다행히 ollama와 Open-WebUI 모두 공식 Container Image가 공개되어 있어서 Docker로 실행하는 데엔 어려움이 없겠는데요.\n하지만 로컬 LLM을 사용하고 종료할 때마다 이 툴들의 Container Image를 실행하고 다시 종료하려면 손이 많이 갈 것 같습니다.\n그래서 여러 Container를 한 번에 배포할 수 있는 Docker Compose를 활용하도록 하겠습니다.\nDocker Compose는 한 개 이상의 Container를 항상 동일한 옵션과 조건으로 한 번에 실행할 수 있도록 도와주는 기능입니다. Container 실행에 필요한 각종 정보를 compose.yaml이라는 파일에 정의해두었다가, docker compose 명령어를 실행하면 yaml 파일에 정의된 Container들이 실행되는 방식입니다.\n배포 과정\n먼저 아래와 같이 Docker Compose 파일을 정의합니다.\nservices:\n  openWebUI:\n    image: ghcr.io/open-webui/open-webui:main\n    restart: always\n    ports:\n      - &quot;3000:8080&quot;\n    extra_hosts:\n      - &quot;host.docker.internal:host-gateway&quot;\n    volumes:\n      - open-webui-local:/app/backend/data\n \n  ollama:\n    image: ollama/ollama:0.1.34\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama\n \nvolumes:\n  ollama-local:\n    external: true\n  open-webui-local:\n    external: true\n다음은 Docker Volume 생성입니다. Volume은 Container 동작 중에 생성/수정되는 데이터를 저장하는 공간인데요.\n위 compose.yaml에서 정의한 바와 같이, ollama-local(ollama의 데이터 저장)와 open-webui-local(Open-WehUI의 데이터 저장)라는 이름의 Docker volume을 생성하기 위해 터미널에서 아래 명령어를 실행합니다.\n\ndocker volume create ollama-local\ndocker volume create open-webui-local\n\n이제 docker compose 명령어로 두 개의 Container를 로컬에 배포해볼 건데요. 그 전에 compose.yaml 파일이 정상적으로 실행되는지 확인하기 위해 아래 명령어로 dry run을 해보겠습니다. (dry-run은 어떤 명령어가 예상대로 동작하는지 모의 실행하는 것을 말합니다. 해당 명령어가 실제로 실행되는 것은 아닙니다.)\n\ndocker compose --dry-run up -d (compose.yaml 파일이 존재하는 경로에서 실행)\n\n\ndry run이 잘 실행되는 것을 확인했으니 이제 아래 명령어를 실행하여 실제로 로컬 배포를 진행해보겠습니다.\n\ndocker compose up -d (compose.yaml 파일이 존재하는 경로에서 실행)\n\n\n각 Container가 정상 실행되었다는 메시지를 확인 후, compose.yaml에서 정의한 Open-WebUI의 Port 번호를 참고하여 웹 브라우저에서 localhost로 접속합니다. (본 예제에서 Open-WebUI 경로: http://localhost:3000)\n\n웹 브라우저로 접속한 Open-WebUI 창에서 Sign up 버튼을 눌러 계정을 새로 만들고 접속합니다. (이렇게 만든 계정은 우리가 이전에 생성한 Open-WebUI의 Docker Volume에 저장되므로 Sign up은 최초 한 번만 필요하며, 이후엔 계정으로 로그인하면 됩니다.)\n\n아직 ollama에서 사용할 LLM 모델이 없으므로, Open-WebUI의 오른쪽 상단의 톱니바퀴 버튼을 누른 뒤 models 메뉴 내 Pull a model from Ollama.com 옵션 입력창에 원하는 LLM 모델의 태그를 입력합니다. (본 예제에서는 llama3:8b를 가져왔습니다. ollama에서 제공하는 LLM 목록은 여기서 확인 가능합니다.)\n\nLLM 모델 다운로드가 완료되면 홈 화면의 왼쪽 상단에서 다운로드한 모델 선택이 가능하고, 이후 Chat을 진행할 수 있습니다.\n\n\n로컬 배포한 Container 관리\n만약 Docker Compose로 로컬 배포한 ollama와 Open-WebUI Container를 종료하고 싶다면 아래 명령어를 실행합니다.\n\ndocker compose down (compose.yaml 파일이 존재하는 경로에서 실행)\n\n추후에 용량 관리를 위해 ollama와 Open-WebUI가 사용하던 Volume을 삭제하고 싶다면 아래 명령어를 실행합니다. (Backup하지 않은 Volume은 삭제 후 복구할 수 없습니다.)\n\ndocker volume rm {대상 Volume 이름}\n\nReferences\n\ngithub.com/ollama/ollama\ngithub.com/open-webui/open-webui\n"},"blog/ollama와-crewAI로-로컬-환경에-블로그-포스팅-시스템-구축":{"title":"ollama와 crewAI로 로컬 환경에 블로그 포스팅 시스템 구축","links":["blog/ollama와-Open-WebUI-로컬-배포"],"tags":["Ollama","CrewAI"],"content":"🦙🧑‍🤝‍🧑Ollama와 CrewAI\nOllama는 로컬 환경에서 LLM을 실행하는 오픈소스 툴입니다. 지난 글에서 Docker로 Ollama와 Open-WebUI라는 툴을 실행해서  웹 브라우저로 로컬 LLM에게 질문을 해보는 튜토리얼을 진행한 적이 있었죠. (관련 블로그 글)\n이번엔 Ollama와 CrewAI를 활용해서 로컬 LLM 기반으로 블로그 글을 작성해주는 시스템을 구축해보려합니다.\nOllama만으로도 충분히 블로그 글을 자동으로 작성할 수 있지 않냐고요? 물론 Ollama로 실행한 LLM에게 부탁해도 글을 써줍니다. 하지만 CrewAI라는 툴을 사용하면, 각자의 역할과 목표를 가지고 있는 여러 LLM 기반 작업자(에이전트)가 일련의 프로세스를 거쳐 더욱 체계적으로 글을 써줄 수 있거든요.\n\n방금 이야기한 블로그 글 작성 시스템을 예로 들면서 알아보겠습니다.\nDevOps 관련 블로그 글을 쓸 때는 보통 아래와 같은 프로세스로 진행이 될 텐데요.\n\n인터넷 자료 조사\n조사한 내용을 토대로 글쓰기\n작성한 글에 오탈자는 없는지 검수하기\n\n이러한 각 과정을 수행하는 에이전트들을 둬서 서로 상호작용하며 작업을 수행하도록 시스템을 만드는 것이 CrewAI의 역할입니다.\n게다가 CrewAI는 파이썬 기반으로 개발되었고 직관적인 명령어들을 사용하기 때문에, 쉽고 빠르게 여러 에이전트로 구성된(Multi-Agent) 작업 수행 시스템을 구축할 수 있다는 장점도 있습니다.\nCrewAI의 에이전트는 역할, 목표, 배경으로 정의하는데요. 각 에이전트가 작업을 수행할 때 자신은 어떤 배경을 가지고 있고, 어떤 목표를 수행하는지 등을 미리 알려주는 거죠.\n\n위의 블로그 글 작성 시스템으로 다시 돌아와서, 위에서 언급한 프로세스의 3가지 작업을 담당하는 CrewAI 에이전트들을 아래처럼 정의해보겠습니다.\n\n인터넷 자료 조사\n\n역할: Researcher\n목표: 최신 DevOps 관련 토픽 조사\n배경: IT 대기업에서 근무 중인 세계적인 Researcher\n\n\n글쓰기\n\n역할: Writer\n목표: DevOps 관련 블로그 글 작성\n배경: IT 관련 글 작성에 특화된 최고의 Technical Writer\n\n\n검수하기\n\n역할: Proofreader\n목표: 기술 블로그 글 검수\n배경: IT 분야에 특화된 유명 Proofreader\n\n\n\n그리고 각 수행되어야 하는 작업도 아래와 같이 정의할 수 있습니다.\n\n최신 DevOps 관련 뉴스 조사\n\n담당 에이전트: Researcher\n출력물 설명: 약 3문단 분량의 최신 DevOps 관련 리포트\n\n\n조사 리포트를 기반으로 DevOps 관련 블로그 글 한 편 작성\n\n담당 에이전트: Writer\n출력물 설명: 약 4문단 분량의 Markdown 형식 DevOps 관련 블로그 글\n\n\n제공된 블로그 글을 보다 자연스럽게 검수\n\n담당 에이전트: Proofreader\n출력물 설명: 약 4문단 분량의 Markdown 형식 DevOps 관련 블로그 글\n\n\n\n\n웹 접근 관련 유의사항\n로컬 LLM을 사용하는 상황에서 웹 접근이 필요한 Researcher 같은 경우엔 Google search API 서비스 등을 별도로 이용해야 합니다.\n그래서 이번 실습에선 카드 등록 없이 이메일 등록으로 최대 2,500회 Google Search 쿼리가 가능한 Serper 서비스를 이용했습니다.\n\n🖥️Ollama와 CrewAI로 블로그 글 작성 시스템 구축하기\n이제 로컬에서 직접 Ollama와 CrewAI를 실행해서 블로그 글 작성 시스템을 구축해보도록 하겠습니다. 각 툴은 Docker로 로컬에 배포합니다.\n이번 실습에선 Ollama의 llama3(8b) 모델을 활용할 예정인데요. 그럴려면 먼저 Ollama를 이용해서 llama3 모델을 로컬에 가져와야겠죠.\nOllama의 모델이 저장될 공간인 Docker volume을 먼저 아래 명령어로 생성합니다.\ndocker volume create ollama-local\nDocker volume을 생성했다면 이제 compose.yaml라는 이름의  Docker compose 파일을 생성하겠습니다. Docker compose는 여러 Docker 컨테이너의 배포 설정을 쉽게 관리하고 실행할 수 있도록 정의하는 파일인데요. 지금은 우선 Ollama에 대해서만 정의해보겠습니다.\ncompose.yamlservices:\n  ollama:\n    image: ollama/ollama:0.1.34\n    container_name: ollama\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama #LLM이 저장될 Volume 지정\nvolumes:\n  ollama-local:\n    external: true\ncompose.yaml 작성이 끝나면 해당 파일이 있는 경로의 터미널에서 아래 명령어로 Docker compose를 실행하겠습니다. 지금은 Docker compose 파일에 정의되어 있는 Ollama만 실행되겠죠?\ndocker compose up -d\nOllama가 정상 실행되었다면 터미널에 아래처럼 표시가 될 겁니다.\n\n이제 실행된 Ollama 컨테이너에 접속해서 우리가 사용할 llama3 LLM을 가져오겠습니다.\n먼저 아래 터미널 명령어로 Ollama 컨테이너에 접속합니다.\ndocker exec -it ollama bash\n접속한 터미널은 아래와 유사한 모습일 겁니다.\n\n이 상태에서 아래 Ollama 명령어를 입력해서 공개된 원격 저장소에서 LLM을 가져옵니다.\nollama pull llama3:8b\n위 명령어를 입력하면 아래처럼 LLM을 가져오는데요. 가져온 LLM은 처음에 생성했던 Docker volume ollama-local에 저장됩니다.\n\ncompose.yaml 파일에서 Ollama 컨테이너와 ollama-local volume 연동 설정을 넣어두었기 때문에, compose.yaml 파일로 실행하는 Ollama는 llama3:8b LLM을 계속 사용할 수 있게 됩니다.\nOllama로 LLM 설치는 완료되었으니, exit 명령어로 컨테이너에서 나옵니다.\n이제 CrewAI를 Docker로 실행해볼 건데요. CrewAI는 파이썬 패키지이므로, 우리가 작성해야 할 파일은 아래와 같이 총 3가지입니다.\n\nCrewAI를 Docker에서 실행하기 위한 Dockerfile(crewai.Dockerfile)\nCrewAI 관련 설정과 정의 후 실행하는 Python 스크립트(main-crewai.py)\nmain-crewai.py 실행에 필요한 패키지를 정의한 requirements.txt\n\ncrewai.DockerfileFROM python:3.12.4\n \nWORKDIR /app\nCOPY requirements.txt ./requirements.txt\nRUN pip install -r requirements.txt\n \nCOPY main-crewai.py ./\n \nCMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;main-crewai.py&quot; ]\nmain-crewai.pyimport os\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai_tools import SerperDevTool\n \n# 로컬에 실행 중인 ollama에서 원하는 LLM 가져옴. 예제에선 llama3 (파라미터 사이즈 8b) 사용\nfrom langchain.llms import Ollama\nollama_model = Ollama(\n    base_url=&#039;http://ollama:11434&#039;,\n    model=&quot;llama3:8b&quot;)\n \nos.environ[&quot;OTEL_SDK_DISABLED&quot;] = &quot;true&quot;\n \n# researcher Agent가 웹에 접근해서 최신 IT 정보를 찾을 수 있도록 server.dev API 서비스 이용\nos.environ[&quot;SERPER_API_KEY&quot;] = &quot;{자신의 serper API key를 넣어주세요}&quot;  # serper.dev API key\nsearch_tool = SerperDevTool()\n \n# crewai 패키지로 원하는 Agent의 역할(role)과 목표(goal) 설정\n# 최신 DevOps 관련 토픽을 조사하는 Agent 정의\nresearcher = Agent(\n    role=&#039;Researcher&#039;,\n    goal=&#039;Discover a newest and attracting topic about DevOps&#039;,\n    backstory=&quot;You&#039;re world class researcher working on a big IT company&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model,\n    tools=[search_tool]\n)\n \n# 블로그 글을 작성하는 Agent 정의\nwriter = Agent(\n    role=&#039;Writer&#039;,\n    goal=&#039;Create DevOps blog post&#039;,\n    backstory=&quot;You&#039;re a best technical writer who is specialized on writing IT content&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model\n)\n \n# 작성된 글을 검수하는 Agent 정의\nproofreader = Agent(\n    role=&#039;Proofreader&#039;,\n    goal=&#039;Edit and proofread technical article&#039;,\n    backstory=&quot;You&#039;re a famous proofreader who is specialized on IT domain&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model\n)\n \n# 정의한 Agent들로 수행할 작업(Task) 정의\nresearch_task = Task(\n    description=&#039;Investigate the latest DevOps news&#039;,\n    agent=researcher,\n    expected_output = &#039;A comprehensive 3 paragraphs long report on the latest and famous DevOps.&#039;\n)\n \nwriting_task = Task(\n    description=&#039;Write a blog post about DevOps with one topic provided from the researcher&#039;,\n    agent=writer,\n    expected_output=&#039;A 4 paragraph article about DevOps formatted as markdown.&#039;,    \n)\n \nproofreading_task = Task(\n    description=&#039;Proofread the provided blog post to make more natural article&#039;,\n    agent=proofreader,\n    expected_output=&#039;A 4 paragraph article about DevOps formatted as markdown.&#039;,    \n)\n \n# 위 Agent와 Task, 작업 프로세스를 정의\ncrew = Crew(\n  agents=[researcher, writer, proofreader],\n  tasks=[research_task, writing_task, proofreading_task],\n  llm=ollama_model,\n  verbose=2, # crew 작업 중에 발생하는 로그의 자세한 정도를 설정 가능.\n  process=Process.sequential # Task가 순차적으로 실행될 수 있도록 sequential로 정의.\n)\n \n# 정의한 crew 실행 및 작업 과정에서 발생하는 로그 출력\nresult = crew.kickoff()\nprint(result)\nrequirements.txtcrewai==0.32.0\ncrewai-tools==0.2.6\nlangchain==0.1.20\nCrewAI 관련 파일 준비가 끝났다면, 아래 Docker 명령어로 CrewAI가 실행될 Docker 이미지를 생성합니다.\ndocker build -t my-crewai -f crewai.Dockerfile .\nCrewAI Docker 이미지 빌드가 끝났다면, 위에서 정의했던 compose.yaml 파일을 아래와 같이 최신화합니다.\ncompose.yamlservices:\n  ollama:\n    image: ollama/ollama:0.1.34\n    container_name: ollama\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama\n  crewai:\n    image: my-crewai\n    container_name: crewai\n    depends_on:\n      - ollama\n    extra_hosts:\n      - &quot;telemetry.crewai.com:127.0.0.1&quot; # To avoid &#039;Connection to telemetry.crewai.com timed out&#039; error when using local LLM\nvolumes:\n  ollama-local:\n    external: true\n이제 터미널에서 아래 Docker compose 명령어를 입력하면, CrewAI의 각 에이전트가 작업을 수행하는 과정과 최종 결과물을 터미널에서 확인할 수 있습니다. (명령어 마지막에 docker compose down을 연결한 것은 CrewAI 작업이 모두 완료되면 Ollama와 CrewAI 컨테이너 모두 정상 종료시키기 위함입니다.)\ndocker compose up -d &amp;&amp; docker compose logs crewai -f &amp;&amp; docker compose down\n🗂️CrewAI의 에이전트들의 작업 과정과 최종 생성 결과물\n이렇게 실행한 CrewAI의 로그를 살펴보면, 각 에이전트가 일하는 과정을 로그로 확인할 수 있습니다.\n\n\n또한 에이전트가 내놓은 최종 결과물도 확인할 수 있죠.\n\nCrewAI의 Researcher 에이전트가 작성한 리포트를 토대로 Writer 에이전트가 블로그 글을 써주는 등, 각 에이전트가 미리 정의된 프로세스대로 상호작용하는 것을 보니 정말 흥미로웠는데요.😄\n블로그 글 작성 외에도 CrewAI를 활용해서 어떤 작업 프로세스를 수행할 수 있을지 궁금해지네요.😊\nReferences\n\ndocs.crewai.com/\nfossengineer.com/ai-agents-crewai/#building-the-crewai-container\n# Create a Blog Writer Multi-Agent System using Crewai and Ollama\n"},"blog/무중단-배포의-종류와-설명":{"title":"무중단 배포의 종류와 설명","links":[],"tags":["Deployment"],"content":"무중단 배포의 종류\n무중단 배포는 아래와 같이 크게 3가지 방식으로 나뉩니다.\nRolling 방식\n동작 중인 인스턴스를 점진적으로 업데이트하는 방식입니다.\n\n\n장점:\n\n인스턴스를 추가로 늘리지 않아도 괜찮습니다.\n인스턴스마다 차례로 버전이 전환되기 때문에 상황에 따라 롤백이 가능합니다.\n\n\n\n단점:\n\n신버전을 배포하고 구버전의 인스턴스 수가 감소하면서 사용 중인 인스턴스에 트래픽이 몰릴 수 있습니다.\n배포 과정에서 구버전과 신버전이 동시에 존재하는 시점이 생기고, 이때 사용자들이 통일되고 균일한 서비스를 받지 못하게 된다.\n\n\n\nBlue / Green 방식\n동작 중인 인스턴스 환경과 동일한 환경에서 새로운 버전을 배포한 뒤, 로드밸런서를 통해 모든 트래픽을 새로운 버전의 인스턴스 환경으로 한 번에 전환하는 방식입니다.\n\n\n장점:\n\n구버전의 인스턴스가 그대로 남아있기 때문에 롤백하기 쉽습니다.\n새 버전의 테스트가 용이합니다.\n\n\n\n단점:\n\n인스턴스 가동에 필요한 시스템 자원이 두 배로 필요합니다.\n인스턴스를 새로 가동하는 환경에 대한 테스트를 사전에 완료해야 합니다.\n\n\n\nCanary 방식\n동작 중인 인스턴스 환경과 동일한 환경에서 새로운 버전을 배포한 뒤, 소수의 사용자 트래픽을 새로운 버전으로 보내 문제가 없음을 확인합니다.\n문제가 없다면 점점 더 많은 사용자 트래픽을 새로운 버전으로 전달하는 방식입니다.\n참고로 Canary(카나리)라는 이름은, 광부들이 유독 가스에 민감한 ‘카나리아’라는 새를 자신들의 작업 환경에 미리 풀어 가스 누출 여부를 감지했던 것에서 유래되었습니다.\n\n\n장점:\n\nA/B 테스트로 활용가능합니다.\n\n\n\n단점:\n\n네트워크 트래픽 제어 작업이 추가로 필요합니다.\n\n\n"},"blog/보안에-강한-Dockerfile-작성-팁-5가지":{"title":"보안에 강한 Dockerfile 작성 팁 5가지","links":[],"tags":["Docker","Security"],"content":"\n들어가기\nDockerfile은 Docker 컨테이너 이미지를 빌드할 때 사용되는 각종 설정과 명령어를 선언한 파일입니다. 우리가 개발 환경을 구축하거나 서비스를 배포할 때 누군가 미리 만들어놓은 Docker 이미지를 사용하기도 하지만, 직접 Dockerfile을 작성해서 Docker 이미지로 빌드하는 경우도 많은데요.\n더욱 안전한 Docker 이미지를 제작할 수 있는 Dockerfile 작성 팁이 있다는 사실, 알고 계셨나요?\nDockerfile을 작성할 때 보안을 신경써야 하는 이유\n오늘날 대부분의 웹 서비스는 마이크로서비스 아키텍처를 따르고 있습니다. 하나의 커다란 서비스를 배포하는 것이 아닌, 기능이나 성격에 따라 나눠진 작은 서비스 여러 개를 배포 및 운영하는 방식을 마이크로서비스 아키텍처라고 하는데요.\n이때 마이크로서비스를 각각의 Docker 컨테이너 내부에서 동작하도록 구성하는 것이 일반적입니다. 하지만 이렇게 실제로 배포된 컨테이너 이미지가 외부 공격에 취약하다면… 생각만 해도 아찔한 보안 사고로 이어지겠죠.\n그래서 우리는 Dockerfile을 작성할 때부터 보안에 신경써야 합니다.\n보안을 위한 Dockerfile 작성 팁 5가지\n그렇다면 Dockerfile을 어떻게 작성하면 좋을까요? 여기 보안에 강한 Dockerfile 작성 팁팁 5가지를 소개해드립니다.\nMulti-Stage 방식으로 빌드\nDockerfile 내에서 애플리케이션 빌드 명령어 실행 후 나오는 최종 산출물을 또다른 Base Image로 복사하고, 해당 Base Image를 최종 Docker Image로 빌드하는 기법을 Multi-Stage라고 하는데요.\nMulti-Stage 방식으로 빌드된 Docker 이미지 내에는 애플리케이션 구동에 필요한 최소한의 요소만 담겨있으므로, 자연스럽게 잠재된 보안 취약점도 더 적어집니다.\nMulti-Stage를 사용해서 간단한 Golang 애플리케이션을 빌드하는 Dockerfile 예제를 같이 살펴보겠습니다.\n# /src/main.go 파일을 빌드하는 build 스테이지입니다.\nFROM golang:1.21 as build \nWORKDIR /src\nCOPY &lt;&lt;EOF /src/main.go\npackage main\n \nimport &quot;fmt&quot;\n \nfunc main() {\n  fmt.Println(&quot;hello, world&quot;)\n}\nEOF\nRUN go build -o /bin/hello ./main.go\n \n# build 스테이지로부터 빌드된 산출물들만 가져와 실행하는 최종 스테이지입니다.\nFROM scratch\nCOPY --from=build /bin/hello /bin/hello\nCMD [&quot;/bin/hello&quot;]\n위 Dockerfile을 실행하면 먼저 build라는 이름이 붙여진 스테이지가 먼저 실행되는데요. golang:1.21 이미지 위에 go build 명령어로 애플리케이션이 빌드되는 구간입니다.\n이후 또다른 스테이지가 실행되고, build 스테이지에서 빌드된 최종 산출물을 복사한 뒤 실행하는 로직이 수행되는데요. 위 Dockerfile로 빌드되는 이미지는 이 최종 스테이지의 내용만 담게 되는 것입니다.\n필요없는 패키지 제거\nDocker 컨테이너 내 애플리케이션 구동에 필요없는 패키지를 제거하는 것 역시 잠재 보안 취약점을 줄여주기 때문에 권장됩니다.\nRoot가 아닌 별도의 사용자를 생성 및 사용\nContainer 내에 Root 계정이 사용될 경우, 침입 사고 발생 시 피해가 커질 수 있습니다. 그래서 아래 예시와 같이 Root 계정이 아닌 별도의 그룹 및 사용자를 생성하고 사용하는 것이 안전합니다.\nRUN groupadd -r for-example &amp;&amp; useradd -r -g for-example for-example\nUSER for-example\nContainer 내 민감한 파일들은 Read Only로\n배포 이후 Docker Container 내에서 추후 수정이 필요하지 않는 파일들을 Read Only로 설정하면 보안성을 높일 수 있습니다.\nchmod -R a-w {폴더명} 또는 chmod a-w {파일명} 명령어를 사용하면, 모든 사용자는 해당 폴더 또는 파일에 대한 쓰기 권한을 잃게 됩니다.\nShell Access 제거\n컨테이너 안에서 실행 가능한 sh나 bash 등의 Shell은 동작 중인 컨테이너 내 애플리케이션을 디버깅할 때 사용되는 경우가 있지만, 이런 통로를 제거하면 컨테이너 내부 침입이 어려워져 보안성이 높아집니다.\nShell 접근을 제거할 경우, 아래 주의사항에 대해 고려해야 합니다.\n\n컨테이너 이미지에서 동작하는 애플리케이션이 Shell을 사용하고 있지 않은지 확인해야 합니다.\n해당 이미지를 배포한 뒤에 디버깅할 수 있는 다른 대안을 미리 마련해야 합니다.\n\n컨테이너 보안을 지킬 수 있는 또다른 방법\nDocker 이미지를 직접 제작할 땐 위와 같은 방법으로 안전한 이미지를 만들어 사용할 수 있지만, 만약 이미 누군가가 제작한 Docker 이미지를 사용할 때엔 그 이미지가 안전한지 어떻게 알 수 있을까요?\n이럴 때 사용할 수 있는 것이 바로, 컨테이너 보안 스캐닝 툴입니다.\n대표적으로 Trivy가 있는데요. 컨테이너 보안 스캐닝 툴에 대해서는 추후 다른 글에서 소개해보겠습니다.\nReferences\n\n# Best practices for writing Dockerfiles\n"},"blog/컨테이너-런타임-보안":{"title":"컨테이너 런타임 보안","links":[],"tags":["Security"],"content":"\nContainer Runtime Security(컨테이너 런타임 보안)이란 컨테이너화된 애플리케이션이 동작 중일 때 보안성을 높이기 위한 활동들을 의미합니다.\n컨테이너 런타임 보안이 중요한 이유\n컨테이너 내부의 애플리케이션은 실제로 동작하는 개체이면서, 대부분 데이터를 처리하는 역할을 수행하기 때문에 보안 공격 대상이 되기 쉽습니다.\n게다가 컨테이너는 호스트의 커널을 공유하기 때문에, 공격자가 컨테이너 내부 애플리케이션에 무단 침입 후 컨테이너 외부로 빠져나오게 된다면 해당 호스트나 다른 컨테이너에도 접근하게 되므로 심각한 보안 사고로 이어질 수 있습니다.\n그렇기 때문에 동작 중인 컨테이너에 대한 보안은 중요하다고 할 수 있겠습니다.\n컨테이너 런타임 보안 활동이 이뤄지는 방식\n이러한 컨테이너 런타임 보안 활동에는 컨테이너에 대한 실시간 모니터링 및 보호 활동 등이 포함됩니다.\n컨테이너가 실행되면서 발생하는 Linux 커널의 System Call을 감시하는 것이 일반적이라고 할 수 있겠습니다.\n이렇게 감시를 진행하다가 프로세스의 이상 행위나 잠재적인 보안 유해 행위가 감지된다면 이를 사용자 또는 관리자에게 알리는데, 더 나아가 미리 정의된 규칙에 따라 이러한 행위를 제한하는 경우도 있습니다.\n컨테이너 런타임 보안을 위해 개발된 툴\n컨테이너 런타임 보안 활동을 위해 개발된 툴에는 여러가지가 있습니다. 그 중 대표적인 3가지를 간략히 소개해드리겠습니다.\n\nFalco\n\nLinux 커널 및 Kubernetes API Call을 통해 노드 및 컨테이너 내 이상 행위를 감지할 수 있는 툴 (관련 글)\n\n\nAqua Security\n\n컨테이너의 런타임 보호 기능을 제공하며, 보안 제어 자동화를 도와줌\n\n\nSysdig Secure\n\n런타임 보안, 포렌식, 취약점 관리 기능 등을 제공함\n\n\n\nReferences\n\nwww.wiz.io/academy/container-runtime-security\nIntroduction: what is container runtime security?\n"},"blog/컨테이너-보안-스캐닝":{"title":"컨테이너 보안 스캐닝","links":[],"tags":["Security"],"content":"컨테이너 보안 스캐닝(Container Security Scanning)이란 컨테이너 이미지에 존재할 수 있는 보안 취약점이나 이슈를 컨테이너 스캐닝 툴로 검사하고 분석하는 행위를 말합니다.\n\n컨테이너 스캐닝이 필요한 이유\n현재 운영 중인 서비스 대부분이 컨테이너 이미지 기반으로 동작하기 때문에 컨테이너 이미지에 보안 취약점 또는 이슈가 있다면 엄청난 보안 사고 및 재정적 피해로 이어질 수 있습니다.\n컨테이너 보안 스캐닝을 수행하면 개발자가 해당 컨테이너에 어떤 취약점이 존재하는지 인지할 수 있고 사전에 조치를 취할 수 있게 됩니다.\n또한, 실제 서비스에 대해서 주기적으로 컨테이너 보안 스캐닝을 수행할 경우, 최종 사용자에게 해당 서비스가 계속 모니터링되어 안전하다는 신뢰감도 줄 수 있습니다.\n컨테이너 스캐닝이 수행되는 방식\n컨테이너 스캐닝 툴은 보통 원격 레지스트리 또는 로컬에 저장된 컨테이너 이미지를 대상으로 스캔합니다.\n이때 스캐닝 툴은 스캔을 위해 해당 이미지를 레이어(Layer) 단위로 해체하는데, Base 이미지와 애플리케이션 코드, 각 디펜던시 역시 해체 대상에 포함됩니다.\n스캐닝 툴의 스캔 방식은 아래와 같이 크게 2가지로 나뉩니다.\n\n지식 기반(Signature-Based) 스캔: 기존에 알려진 취약점(CVE 등)을 기반으로 스캔 수행\n행동 기반(Behavioral-Based) 스캔: 컨테이너 실행 시점에 비정상적인 프로세스나 네트워크 트래픽 같은 이상 행동에 대해 스캔 수행\n\n컨테이너 보안 스캐닝 툴\n\nTrivy: 컨테이너 이미지, 파일 시스템, Kubernetes 클러스터 등 다양한 대상에 대해 보안 이슈와 취약점 스캔 가능합니다. CLI로 스캔 작업을 실행합니다. (관련 문서)\nClair: 함께 설치되는 DB에 스캔 대상 컨테이너 이미지에 대한 정보와 레이어를 저장 후 보안 취약점을 스캔하는 것이 특징입니다. CLI로 스캔 작업을 실행하며, Webhook을 통한 알림 기능도 지원합니다. (관련 문서)\n\nReferences\n\nwww.wiz.io/academy/container-security-scanning\nsnyk.io/learn/container-security/container-scanning\n"},"blog/컨테이너-엔진-관련-취약점-(CVE-2024-21626)":{"title":"컨테이너 엔진 관련 취약점: CVE-2024-21626","links":[],"tags":["CVE","Container"],"content":"2024년 1월 말, 컨테이너 엔진으로 널리 쓰이는 runc에서 취약점(CVE-2024-21626)이 발견되었습니다. runc는 Docker와 Kubernetes 등이 컨테이너 이미지를 빌드하거나 실행할 때 사용하는 컨테이너 엔진입니다.\n\nCVE-2024-21626 취약점이란\n\n공격자가 컨테이너에서 탈출(Container Escape)하여 호스트 시스템에 비허가 접근 위험이 있는 취약점(CVE-2024-21626)이 runc에서 발견되었습니다.\n해당 취약점은 runc가 컨테이너 이미지를 생성하거나 실행하는 특정 시점에 호스트 내 파일레 접근 가능한 통로가 여전히 남아있기 때문에 발생한 것입니다.\n악성 컨테이너 이미지를 실행하거나, 악성 Dockerfile 또는 악성 Base Image로 컨테이너 이미지를 빌드할 때 해당 취약점의 영향을 받을 수 있습니다.\n\nCVE-2024-21626이 위험한 이유\n\n공격자가 컨테이너에서 벗어나 호스트 시스템에 저장되어있던 모든 데이터(계정 정보, 사용자 정보 등)를 획득 가능합니다.\n또한 공격자가 컨테이너에서 벗어난 뒤 호스트 시스템에서 추가 공격이 가능합니다.\n컨테이너 엔진인 runc에 영향을 주는 취약점이므로, runc를 기반으로 작동하면서 널리 사용되는 컨테이너 툴 Docker, Kubernetes도 영향을 받게 되어 파급력이 큽니다.\n\nCVE-2024-21626에 대한 대처 방법\n\n\n사용 중인 컨테이너 툴 버전을 해당 취약점에 대해 보완된 버전으로 최신 업데이트합니다.\n\nrunc &gt;= 1.1.12\ncontainerd &gt;= 1.6.28\nDocker Desktop &gt;= 4.27.1\n\n\n\n현재 사용 중인 컨테이너 툴을 바로 업데이트하기 어렵다면…\n\n신뢰할 수 있는 컨테이너 이미지만 사용합니다.\n컨테이너 이미지 빌드 시, 신뢰할 수 있는 Dockerfile 또는 신뢰할 수 있는 Base Image만 사용합니다.\n\n\n"},"index":{"title":"🔭 DevOps 여행을 위한 안내서","links":[],"tags":[],"content":"\nAiden Kim\nDevOps Engineer / Kubestronaut\n💻Working as a DevOps engineer\n☁️Passionate in Cloud-native ecosystem\n✨️Interested in sharing DevOps and Cloud knowledge\n안녕하세요, DevOps와 클라우드 관련 지식을 공유하는 DevOps 여행을 위한 안내서에 오신 것을 환영합니다.\n뉴스레터 DevOps 여행을 위한 소식지를 구독하시면 DevOps 안내서의 글을 보다 빠르게 받아보실 수 있습니다.\nContact\nEmail: edu.ukulelekim@gmail.com"}}